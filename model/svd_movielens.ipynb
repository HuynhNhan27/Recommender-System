{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<i>Copyright (c) Recommenders contributors.</i>\n",
                "\n",
                "<i>Licensed under the MIT License.</i>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Surprise Singular Value Decomposition (SVD)\n",
                "\n",
                "This notebook implements the Singular Value Decomposition (SVD) algorithm, a popular collaborative filtering technique. SVD was notably successful during the Netflix Prize competition, used by the winning BellKor team. It aims to discover latent factors that explain user-item interactions (ratings)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import surprise\n",
                "\n",
                "from recommenders.utils.timer import Timer\n",
                "from recommenders.datasets import movielens\n",
                "from recommenders.datasets.python_splitters import python_random_split\n",
                "from recommenders.evaluation.python_evaluation import (\n",
                "    rmse,\n",
                "    mae,\n",
                "    rsquared,\n",
                "    exp_var,\n",
                "    map_at_k,\n",
                "    ndcg_at_k,\n",
                "    precision_at_k,\n",
                "    recall_at_k,\n",
                "    get_top_k_items,\n",
                ")\n",
                "from recommenders.models.surprise.surprise_utils import (\n",
                "    predict,\n",
                "    compute_ranking_predictions,\n",
                ")\n",
                "from recommenders.utils.notebook_utils import store_metadata"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This is key variables for the recommendation process, including the number of top items to recommend (`TOP_K`), a specific `user_id` for demonstration, and the size of the Movielens dataset to be used (`MOVIELENS_DATA_SIZE`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "# Top k items to recommend\n",
                "TOP_K = 10\n",
                "# User ID to recommend for\n",
                "user_id = 164\n",
                "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
                "MOVIELENS_DATA_SIZE = \"100k\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Load the dataset into a dataframe\n",
                "This cell loads the Movielens dataset into a Pandas DataFrame using the `movielens.load_pandas_df()` function. The dataset contains user-item-rating triplets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 4.81k/4.81k [00:05<00:00, 911KB/s]  \n"
                    ]
                }
            ],
            "source": [
                "data = movielens.load_pandas_df(\n",
                "    size=MOVIELENS_DATA_SIZE,\n",
                "    header=[\"userID\", \"itemID\", \"rating\"],\n",
                "    local_cache_path=None\n",
                ")\n",
                "# data.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The loaded data is then split into training and testing sets using `python_random_split` from the Recommenders library. The training set is further converted into a Surprise `Trainset` object, which is required by the Surprise library's algorithms."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare the data for training\n",
                "train, test = python_random_split(data, 0.75)\n",
                "train_set = surprise.Dataset.load_from_df(\n",
                "    train, reader=surprise.Reader(\"ml-100k\")\n",
                ").build_full_trainset()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Initialize Model\n",
                "Here, an SVD model from the Surprise library is initialized with specific parameters: `random_state` for reproducibility, `n_factors` controlling the dimensionality of the latent factor space, `n_epochs` specifying the number of training iterations, and `verbose` to display training progress.\n",
                "The model is then trained on the `train_set`, and the training time is recorded using the `Timer` utility."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing epoch 0\n",
                        "Processing epoch 1\n",
                        "Processing epoch 2\n",
                        "Processing epoch 3\n",
                        "Processing epoch 4\n",
                        "Processing epoch 5\n",
                        "Processing epoch 6\n",
                        "Processing epoch 7\n",
                        "Processing epoch 8\n",
                        "Processing epoch 9\n",
                        "Processing epoch 10\n",
                        "Processing epoch 11\n",
                        "Processing epoch 12\n",
                        "Processing epoch 13\n",
                        "Processing epoch 14\n",
                        "Processing epoch 15\n",
                        "Processing epoch 16\n",
                        "Processing epoch 17\n",
                        "Processing epoch 18\n",
                        "Processing epoch 19\n",
                        "Processing epoch 20\n",
                        "Processing epoch 21\n",
                        "Processing epoch 22\n",
                        "Processing epoch 23\n",
                        "Processing epoch 24\n",
                        "Processing epoch 25\n",
                        "Processing epoch 26\n",
                        "Processing epoch 27\n",
                        "Processing epoch 28\n",
                        "Processing epoch 29\n",
                        "Took 6.720439725000688 seconds for training.\n"
                    ]
                }
            ],
            "source": [
                "svd = surprise.SVD(\n",
                "    random_state=0, n_factors=200,\n",
                "    n_epochs=30, verbose=True)\n",
                "with Timer() as train_time:\n",
                "    svd.fit(train_set)\n",
                "\n",
                "print(f\"Took {train_time.interval} seconds for training.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Prediction for user\n",
                "This section generates predictions on the `test` set using the trained SVD model. It also computes ranking predictions for all items for the users in the training set, excluding items they have already interacted with. This is used for evaluating ranking metrics later. The prediction time is also recorded."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Took 55.19310974400105 seconds for predictions.\n"
                    ]
                }
            ],
            "source": [
                "# Predictions on the test set\n",
                "predictions = predict(\n",
                "    svd, test, usercol=\"userID\",\n",
                "    itemcol=\"itemID\"\n",
                ")\n",
                "# Remove seen items from the test set\n",
                "with Timer() as test_time:\n",
                "    all_predictions = compute_ranking_predictions(\n",
                "        svd, train, usercol=\"userID\",\n",
                "        itemcol=\"itemID\", remove_seen=True\n",
                "    )\n",
                "print(f\"Took {test_time.interval} seconds for predictions.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This cell demonstrates how to retrieve the top `TOP_K` (in this case, 10) recommended items for a specific `user_id` (164) based on the ranking predictions generated earlier."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>userID</th>\n",
                            "      <th>itemID</th>\n",
                            "      <th>prediction</th>\n",
                            "      <th>rank</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>164</td>\n",
                            "      <td>50</td>\n",
                            "      <td>5.0</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>164</td>\n",
                            "      <td>126</td>\n",
                            "      <td>5.0</td>\n",
                            "      <td>2</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>164</td>\n",
                            "      <td>134</td>\n",
                            "      <td>5.0</td>\n",
                            "      <td>3</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>164</td>\n",
                            "      <td>169</td>\n",
                            "      <td>5.0</td>\n",
                            "      <td>4</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>164</td>\n",
                            "      <td>189</td>\n",
                            "      <td>5.0</td>\n",
                            "      <td>5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>5</th>\n",
                            "      <td>164</td>\n",
                            "      <td>207</td>\n",
                            "      <td>5.0</td>\n",
                            "      <td>6</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6</th>\n",
                            "      <td>164</td>\n",
                            "      <td>408</td>\n",
                            "      <td>5.0</td>\n",
                            "      <td>7</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7</th>\n",
                            "      <td>164</td>\n",
                            "      <td>430</td>\n",
                            "      <td>5.0</td>\n",
                            "      <td>8</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>8</th>\n",
                            "      <td>164</td>\n",
                            "      <td>480</td>\n",
                            "      <td>5.0</td>\n",
                            "      <td>9</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>9</th>\n",
                            "      <td>164</td>\n",
                            "      <td>483</td>\n",
                            "      <td>5.0</td>\n",
                            "      <td>10</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   userID  itemID  prediction  rank\n",
                            "0     164      50         5.0     1\n",
                            "1     164     126         5.0     2\n",
                            "2     164     134         5.0     3\n",
                            "3     164     169         5.0     4\n",
                            "4     164     189         5.0     5\n",
                            "5     164     207         5.0     6\n",
                            "6     164     408         5.0     7\n",
                            "7     164     430         5.0     8\n",
                            "8     164     480         5.0     9\n",
                            "9     164     483         5.0    10"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "get_top_k_items(\n",
                "    all_predictions[all_predictions[\"userID\"] == user_id],\n",
                "    col_rating=\"prediction\", k=TOP_K\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Evaluate Model\n",
                "This part evaluates the performance of the trained SVD model using various regression metrics (RMSE, MAE, R-squared, Explained Variance) on the `predictions` DataFrame and ranking metrics (MAP@K, NDCG@K, Precision@K, Recall@K) on the `all_predictions` DataFrame. These metrics quantify the accuracy and ranking quality of the recommendations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RMSE:\t\t0.948771\n",
                        "MAE:\t\t0.747003\n",
                        "rsquared:\t0.288045\n",
                        "exp var:\t0.288157\n",
                        "----\n",
                        "MAP:\t\t0.051213\n",
                        "NDCG:\t\t0.109878\n",
                        "Precision@K:\t0.100318\n",
                        "Recall@K:\t0.035359\n"
                    ]
                }
            ],
            "source": [
                "# Evaluation metrics   \n",
                "eval_rmse = rmse(test, predictions)\n",
                "eval_mae = mae(test, predictions)\n",
                "eval_rsquared = rsquared(test, predictions)\n",
                "eval_exp_var = exp_var(test, predictions)\n",
                "\n",
                "eval_map = map_at_k(test, all_predictions, col_prediction=\"prediction\", k=TOP_K)\n",
                "eval_ndcg = ndcg_at_k(test, all_predictions, col_prediction=\"prediction\", k=TOP_K)\n",
                "eval_precision = precision_at_k(\n",
                "    test, all_predictions, col_prediction=\"prediction\", k=TOP_K\n",
                ")\n",
                "eval_recall = recall_at_k(test, all_predictions, col_prediction=\"prediction\", k=TOP_K)\n",
                "\n",
                "\n",
                "print(\n",
                "    \"RMSE:\\t\\t%f\" % eval_rmse,\n",
                "    \"MAE:\\t\\t%f\" % eval_mae,\n",
                "    \"rsquared:\\t%f\" % eval_rsquared,\n",
                "    \"exp var:\\t%f\" % eval_exp_var,\n",
                "    sep=\"\\n\",\n",
                ")\n",
                "print(\"----\")\n",
                "print(\n",
                "    \"MAP:\\t\\t%f\" % eval_map,\n",
                "    \"NDCG:\\t\\t%f\" % eval_ndcg,\n",
                "    \"Precision@K:\\t%f\" % eval_precision,\n",
                "    \"Recall@K:\\t%f\" % eval_recall,\n",
                "    sep=\"\\n\",\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Baseline SVD\n",
                "This section trains a baseline SVD model using the default parameters of the Surprise library. This allows for a comparison of performance against the custom SVD model trained earlier with specific hyperparameters. The training time for the baseline model is also recorded."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Baseline SVD model took 2.800016314999084 seconds for training.\n"
                    ]
                }
            ],
            "source": [
                "# Normal SVD model\n",
                "svd_baseline = surprise.SVD(random_state=0)\n",
                "with Timer() as train_time_baseline:\n",
                "    svd_baseline.fit(train_set)\n",
                "\n",
                "print(f\"Baseline SVD model took {train_time_baseline.interval} seconds for training.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Similar to the custom SVD model, this cell generates predictions on the test set and computes ranking predictions for the baseline SVD model. The prediction time for the baseline model is also recorded."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Baseline SVD model took 44.47100239500105 seconds for prediction.\n"
                    ]
                }
            ],
            "source": [
                "# Predictions with the baseline model\n",
                "predictions_baseline = predict(\n",
                "    svd_baseline, test, usercol=\"userID\",\n",
                "    itemcol=\"itemID\"\n",
                ")\n",
                "with Timer() as test_time_baseline:\n",
                "    all_predictions_baseline = compute_ranking_predictions(\n",
                "        svd_baseline, train, usercol=\"userID\",\n",
                "        itemcol=\"itemID\", remove_seen=True\n",
                "    )\n",
                "print(f\"Baseline SVD model took {test_time_baseline.interval} seconds for prediction.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The performance of the baseline SVD model is evaluated using the same regression and ranking metrics as the custom model. This provides a direct comparison of their effectiveness."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Baseline SVD Model ---\n",
                        "RMSE:\t\t0.936708\n",
                        "MAE:\t\t0.738306\n",
                        "rsquared:\t0.306034\n",
                        "exp var:\t0.306115\n",
                        "----\n",
                        "MAP:\t\t0.050510\n",
                        "NDCG:\t\t0.105416\n",
                        "Precision@K:\t0.100212\n",
                        "Recall@K:\t0.033213\n"
                    ]
                }
            ],
            "source": [
                "# Evaluation metrics for the baseline model\n",
                "eval_rmse_baseline = rmse(test, predictions_baseline)\n",
                "eval_mae_baseline = mae(test, predictions_baseline)\n",
                "eval_rsquared_baseline = rsquared(test, predictions_baseline)\n",
                "eval_exp_var_baseline = exp_var(test, predictions_baseline)\n",
                "\n",
                "eval_map_baseline = map_at_k(test, all_predictions_baseline, col_prediction=\"prediction\", k=TOP_K)\n",
                "eval_ndcg_baseline = ndcg_at_k(test, all_predictions_baseline, col_prediction=\"prediction\", k=TOP_K)\n",
                "eval_precision_baseline = precision_at_k(\n",
                "    test, all_predictions_baseline, col_prediction=\"prediction\", k=TOP_K\n",
                ")\n",
                "eval_recall_baseline = recall_at_k(test, all_predictions_baseline, col_prediction=\"prediction\", k=TOP_K)\n",
                "\n",
                "print(\"\\n--- Baseline SVD Model ---\")\n",
                "print(\n",
                "    \"RMSE:\\t\\t%f\" % eval_rmse_baseline,\n",
                "    \"MAE:\\t\\t%f\" % eval_mae_baseline,\n",
                "    \"rsquared:\\t%f\" % eval_rsquared_baseline,\n",
                "    \"exp var:\\t%f\" % eval_exp_var_baseline,\n",
                "    sep=\"\\n\",\n",
                ")\n",
                "print(\"----\")\n",
                "print(\n",
                "    \"MAP:\\t\\t%f\" % eval_map_baseline,\n",
                "    \"NDCG:\\t\\t%f\" % eval_ndcg_baseline,\n",
                "    \"Precision@K:\\t%f\" % eval_precision_baseline,\n",
                "    \"Recall@K:\\t%f\" % eval_recall_baseline,\n",
                "    sep=\"\\n\",\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. Compare baseline SVD and custom SVD\n",
                "Finally, this section prints a concise comparison of the evaluation metrics for both the custom SVD model and the baseline SVD model, allowing for easy interpretation of the impact of the chosen hyperparameters on the model's performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Compare custom and baseline ---\n",
                        "Custom SVD:\n",
                        "  RMSE: 0.9488, MAE: 0.7470, MAP@10: 0.0512, NDCG@10: 0.1099\n",
                        "Baseline SVD:\n",
                        "  RMSE: 0.9367, MAE: 0.7383, MAP@10: 0.0505, NDCG@10: 0.1054\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n--- Compare custom and baseline ---\")\n",
                "print(\"Custom SVD:\")\n",
                "print(f\"  RMSE: {eval_rmse:.4f}, MAE: {eval_mae:.4f}, MAP@{TOP_K}: {eval_map:.4f}, NDCG@{TOP_K}: {eval_ndcg:.4f}\")\n",
                "print(\"Baseline SVD:\")\n",
                "print(f\"  RMSE: {eval_rmse_baseline:.4f}, MAE: {eval_mae_baseline:.4f}, MAP@{TOP_K}: {eval_map_baseline:.4f}, NDCG@{TOP_K}: {eval_ndcg_baseline:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "celltoolbar": "Tags",
        "kernelspec": {
            "display_name": "recommenders",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.21"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
